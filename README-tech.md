	# ğŸ§© R4Rv5 â€” Technical Overview
	**Conversational Builder + Persistent RAG System**
	
	---
	
	## ğŸ“˜ Ãndice
	1. [Arquitectura general](#-arquitectura-general)
	2. [Backend: Flask + LangChain](#-backend-flask--langchain)
	3. [Frontend: estructura JS modular](#-frontend-estructura-js-modular)
	4. [Pipeline interno de chat](#-pipeline-interno-de-chat)
	5. [Persistencia y backups](#-persistencia-y-backups)
	6. [Contextos & generaciÃ³n de `context.md`](#-contextos--generaciÃ³n-de-contextmd)
	7. [HUD y metadatos de mÃ©trica](#-hud-y-metadatos-de-mÃ©trica)
	8. [ExtensiÃ³n del modelo (OpenAI, HuggingFace, etc.)](#-extensiÃ³n-del-modelo)
	9. [Control de versiones y buenas prÃ¡cticas](#-control-de-versiones--buenas-prÃ¡cticas)
	
	---
	
	## ğŸ§  Arquitectura general
	
	R4Rv5 implementa un sistema de **RAG (Retrievalâ€‘Augmented Generation)** con persistencia local:
	
	```text
	Frontend JS â†’ Flask API â†’ LangChain/Ollama â†’ Modelo LLM
	        â†‘                     â†“
	   Pickle + Context.md   GeneraciÃ³n de respuesta + mÃ©tricas
	```

-----------------------------------------------------
	
Cada proyecto genera su propio ecosistema de contexto:

projects/<slug>/

â”œâ”€â”€ main/contextmemory_main.pkl

â”œâ”€â”€ main/backups/contextmemory_main_YYYYMMDD_HHMMSS.bak

â””â”€â”€ fase N/contextmemory_fase N.pkl

-----------------------------------------------------
	
	## âš™ï¸ Backend Flask + LangChain
	
	Archivo: **`r4r_ui/app.py`**
	
	### Funciones principales
	| Endpoint | DescripciÃ³n |
	|-----------|--------------|
	| `POST /api/message` | Canaliza prompts hacia el modelo activo |
	| `GET /api/projects` | Enumera proyectos y fases |
	| `POST /api/save_context` | Crea `context.md` y la siguiente fase |
	| `POST /api/history` | Devuelve historial persistente de mensajes |
	| `PATCH/DELETE /api/project/<slug>` | Renombra o elimina proyectos |
	
	### Core components
	- **`R4RConversationalRAG`** (en `rag_chain.py`):  
	  - Contiene el `ChatOllama` de LangChain.  
	  - Gestiona la memoria RAM (`ConversationBufferMemory`).  
	  - Indexa contextos mediante `R4RVectorStore`.  
	  - Garantiza persistencia por sesiÃ³n (`.pkl`).
	
	- **`SessionLogger` (conversation_persistence.py)**  
	  - Escribe/lee los `.pkl`.  
	  - Crea backups automÃ¡ticos en cada mensaje.  
	  - Controla corrupciÃ³n o recupera `.bak` automÃ¡ticamente.
	
	---
	
	## ğŸ’» Frontend: estructura JS modular

---------------------------------------------------

r4r_ui/static/js/

â”œâ”€â”€ core/

â”‚    â”œâ”€â”€ apiClient.js       â† comunicaciÃ³n REST

â”‚    â”œâ”€â”€ chatRenderer.js    â† render + loader + HUD

â”‚    â”œâ”€â”€ stateManager.js    â† proyecto/fase actual

â”‚    â””â”€â”€ markdown.js        â† renderer Markdown + HLJS

â””â”€â”€ ui/

â”œâ”€â”€ sidebar.js         â† navegaciÃ³n de proyectos/fases

â”œâ”€â”€ header.js          â† header + guardado

â””â”€â”€ feedback.js        â† sistema de toasts

----------------------------------------------------

	### ComunicaciÃ³n
	Todo el frontend usa `fetch` a `/api/*`  
	y responses en formato JSON:
	```json
	{
	  "reply": "...",
	  "project": "slug_proyecto",
	  "phase": "main",
	  "metrics": {"ttf": 1.23, "tokens": 312, "tok_per_s": 205.4},
	  "model": "mistral:7b-instruct-q4_K_M"
	}


---

ğŸ”„ Pipeline interno de chat

Flujo resumido

	prompt usuario
	   â†“
	apiClient.sendMessage()
	   â†“
	@app.route("/api/message")
	   â†“
	R4RConversationalRAG.query()
	   â†“
	LangChain â†’ ChatOllama.generate()
	   â†“
	respuesta JSON + mÃ©tricas
	   â†“
	chatRenderer.typeResponseIn()
	   â†“
	HUD + persistencia en .pkl


---

ğŸ’¾ Persistencia y backups


Archivo: r4r_core/conversation_persistence.py


- Guardado atÃ³mico: se escribe primero .pkl.tmp, luego se reemplaza.

- Backup rotativo:

cada guardado genera backups/contextmemory_main_YYYYMMDD_HHMMSS.bak.

- RecuperaciÃ³n segura:

si .pkl estÃ¡ vacÃ­o o corrupto â†’ se rehidrata desde el Ãºltimo .bak.

Formato del .pkl:


	[
	  {"role": "user", "content": "Â¿CuÃ¡nto es 2+2?"},
	  {
	    "role": "assistant",
	    "content": "La suma de 2 y 2 es 4.",
	    "meta": {
	      "metrics": {"ttf": 0.8, "tokens": 8, "tok_per_s": 120.5},
	      "model": "mistral:7b-instruct-q4_K_M"
	    }
	  }
	]


---

ğŸŒ Contextos & generaciÃ³n de context.md


Archivo: context_builder.py

Procesa la conversaciÃ³n de cada fase para crear un

archivo context.md con:


- encabezado YAML (title, tags, created, summary)

- cuerpo concatenado de conversaciÃ³n

- detecciÃ³n automÃ¡tica de palabras clave

- resumen semÃ¡ntico (summarizer_chain.py)

Ejemplo:


	---
	title: "MultiplicaciÃ³n BÃ¡sica"
	tags: [main]
	created: 2025-11-17 11:30
	summary: "El usuario solicita operaciones aritmÃ©ticas simples."
	---
	USER: Â¿CuÃ¡nto es 3Ã—4?
	ASSISTANT: El resultado es 12.


---

âš¡ HUD y metadatos de mÃ©trica


HUD = â€œHeadsâ€‘Up Displayâ€ mostrado bajo cada respuesta.

Incluye:


- modelo (meta.model)

- tokens

- velocidad (tok/s)

- timeâ€‘toâ€‘first (TTF)

Es reconstruido automÃ¡ticamente desde m.meta.metrics en chatRenderer.renderAll()

tras un F5 o cambio de proyecto.


---

ğŸ§© ExtensiÃ³n del modelo


Para cambiar de modelo no se modifica cÃ³digo, solo el .env:


	MODEL_PROVIDER=ollama_local
	MODEL_NAME=mistral:7b-instruct-q4_K_M

Otros proveedores

- OpenAI

	MODEL_PROVIDER=openai
	MODEL_NAME=gpt-4o
	OPENAI_API_KEY=<tu_key>



- Anthropic

	MODEL_PROVIDER=anthropic
	MODEL_NAME=claude-3-opus
	ANTHROPIC_API_KEY=<tu_key>



La clase get_llm() de summarizer_chain.py detecta automÃ¡ticamente el proveedor.


---

ğŸ§± Control de versiones & buenas prÃ¡cticas

- Usa Python 3.11+ y LangChain â‰¥ 0.2.x.

- No edites los .pkl manualmente.

- Cada PR debe pasar por los checks de persistencia .pkl y context.md.

- MantÃ©n el .env fuera de commits (.gitignore).


---

ğŸ§‘ğŸ’» AutorÃ­a


Gerard Piella Olmedo â€” Arquitectura, desarrollo, UI/UX

Asistencia tÃ©cnica y refactorizaciÃ³n colaborativa mediante GPTâ€‘5.


---

âš–ï¸ Licencia


MIT License Â© 2025

Puedes reutilizar, modificar y redistribuir libremente,

manteniendo referencia a la autorÃ­a original.

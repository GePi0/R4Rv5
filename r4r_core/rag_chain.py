# r4r_core/rag_chain.py
# -------------------------------------------------------------
# Conversational RAG pipeline con memoria RAM (contextMemory)
# combinada con memoria persistente (.pkl) y bÃºsqueda semÃ¡ntica
# -------------------------------------------------------------

from pathlib import Path
from r4r_core.vector_store import R4RVectorStore
from r4r_core.conversation_persistence import SessionLogger
from langchain_ollama import ChatOllama
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain.memory import ConversationBufferMemory
from dotenv import load_dotenv
import os

class R4RConversationalRAG:
    """
    Pipeline conversacional RAG con tres capas de memoria:
      1. contextMemory (RAM / LangChain)
      2. contextmemory_faseX.pkl  (persistente)
      3. context.md (vector search)
    """

    def __init__(self, project_dir: Path, phase: str):
        load_dotenv()
        self.project_dir = project_dir
        self.phase = phase

        # Modelo LLM desde .env
        self.llm = ChatOllama(
            model=os.getenv("MODEL_NAME", "mistral:7b"),
            temperature=0.3,
            base_url=os.getenv("OLLAMA_API_HOST", "http://localhost:11434"),
        )

        # Core components
        self.memory = ConversationBufferMemory(return_messages=True)
        self.vector_store = R4RVectorStore()
        self.logger = SessionLogger(project_dir, phase)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    #   INIT / HYDRATION
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def initialize(self):
        """Indexa contextos (main + fases) en memoria
        y reconstruye el buffer RAM desde el .pkl existente.
        """
        print(f"ğŸ§© Indexando contextos de {self.project_dir.name} ...")
        self.vector_store.index_contexts(self.project_dir)

        # Rehidratar .pkl â†’ contextMemory (RAM)
        previous_msgs = self.logger.load()
        if previous_msgs:
            for m in previous_msgs:
                role = m["role"].lower()
                content = m["content"]
                if role == "user":
                    self.memory.chat_memory.add_user_message(content)
                else:
                    self.memory.chat_memory.add_ai_message(content)
            print(f"ğŸ” Rehidratada memoria con {len(previous_msgs)} mensajes previos.\n")
        else:
            print("ğŸ†• Nueva sesiÃ³n â€” memoria vacÃ­a.\n")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    #   MAIN QUERY PIPELINE
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def query(self, user_input: str, k: int = 3) -> str:
        # Registrar mensaje
        self.logger.save("user", user_input)
        self.memory.chat_memory.add_user_message(user_input)

        # Paso 1: bÃºsqueda semÃ¡ntica topâ€‘k en context.md
        relevant_docs = self.vector_store.query(user_input, k=k)
        snippets = "\n\n".join(
            [doc.page_content[:800] for doc in relevant_docs]
        )

        # Paso 2: preparar prompt base
        template = PromptTemplate(
            input_variables=["context", "conversation", "question"],
            template=(
                "Eres un asistente dentro del proyecto R4R. "
                "Responde basÃ¡ndote en el contexto del proyecto y la conversaciÃ³n actual.\n\n"
                "Contexto relevante:\n{context}\n\n"
                "ConversaciÃ³n previa:\n{conversation}\n\n"
                "Pregunta: {question}\n\n"
                "Responde de manera clara y en espaÃ±ol:"
            ),
        )

        # Dump temporal del buffer como string
        conversation_text = "\n".join(
            f"{m.type}: {m.content}" for m in self.memory.chat_memory.messages[-8:]
        )

        chain = RunnablePassthrough() | template | self.llm | StrOutputParser()

        response = chain.invoke(
            {
                "context": snippets,
                "conversation": conversation_text,
                "question": user_input,
            }
        )

        # Guardar mensaje en buffer y persistente con metadatos
        self.memory.chat_memory.add_ai_message(response)
        try:
            from os import getenv
            self.logger.save(
                "assistant",
                response,
                {
                    "metrics": {
                        "length": len(response.split())
                    },
                    "model": getenv("MODEL_NAME", "undefined"),
                }
            )
        except Exception as e:
            print(f"[âš ï¸] Error guardando metadatos: {e}")

        return response

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    #   FINALIZACIÃ“N / GUARDADO
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def finalize(self):
        """Resincroniza buffer RAM â†’ .pkl manteniendo los metadatos previos."""
        from pathlib import Path
        import pickle
    
        # 1ï¸âƒ£ Cargar versiÃ³n actual (si existe)
        existing = []
        if self.logger.memory_path.exists():
            try:
                with open(self.logger.memory_path, "rb") as f:
                    existing = pickle.load(f)
            except Exception:
                existing = []
    
        # 2ï¸âƒ£ Reconstruir mensajes del buffer actual
        updated = []
        for m in self.memory.chat_memory.messages:
            role = "user" if m.type == "human" else "assistant"
            content = m.content
            updated.append({"role": role, "content": content})
    
        # 3ï¸âƒ£ Si los existentes tenÃ­an meta (metrics, model),
        #     los transferimos al matching assistant original.
        merged = []
        for idx, msg in enumerate(updated):
            if msg["role"] == "assistant":
                # Recuperar meta del existente si coincide por orden
                meta = None
                if idx < len(existing) and isinstance(existing[idx], dict):
                    meta = existing[idx].get("meta")
                if meta:
                    msg["meta"] = meta
            merged.append(msg)
    
        # 4ï¸âƒ£ Guardar sin perder meta
        tmp_path = self.logger.memory_path.with_suffix(".pkl.tmp")
        with open(tmp_path, "wb") as f:
            pickle.dump(merged, f)
        tmp_path.replace(self.logger.memory_path)
    
        print(f"ğŸ’¾ Memoria sincronizada preservando metadatos â†’ {self.logger.memory_path}")
